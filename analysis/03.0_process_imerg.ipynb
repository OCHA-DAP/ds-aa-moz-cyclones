{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for processing IMERG Rainfall data in Blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to process the rainfall data on Blob for each cyclone since Favio. It extracts the daily on-land rainfall values from -2 days to +5 days to landfall in a 250km radius around the landfall location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyter_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyter_black\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext jupyter_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "from azure.storage.blob import ContainerClient\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "from shapely.geometry import Point\n",
    "from pyproj import CRS, Transformer\n",
    "from shapely.ops import transform\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "load_dotenv()\n",
    "ADMS = [\"Sofala\", \"Inhambane\", \"Nampula\", \"Zambezia\"]\n",
    "AA_DATA_DIR = Path(os.getenv(\"AA_DATA_DIR\"))\n",
    "AA_DATA_DIR_NEW = Path(os.getenv(\"AA_DATA_DIR_NEW\"))\n",
    "\n",
    "DEV_BLOB_SAS = os.getenv(\"DSCI_AZ_SAS_DEV\")\n",
    "DEV_BLOB_NAME = \"imb0chd0dev\"\n",
    "DEV_BLOB_URL = f\"https://{DEV_BLOB_NAME}.blob.core.windows.net/\"\n",
    "DEV_BLOB_PROJ_URL = DEV_BLOB_URL + \"projects\" + \"?\" + DEV_BLOB_SAS\n",
    "GLOBAL_CONTAINER_NAME = \"global\"\n",
    "DEV_BLOB_GLB_URL = DEV_BLOB_URL + GLOBAL_CONTAINER_NAME + \"?\" + DEV_BLOB_SAS\n",
    "\n",
    "dev_glb_container_client = ContainerClient.from_container_url(DEV_BLOB_GLB_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_path = (\n",
    "    AA_DATA_DIR\n",
    "    / \"public\"\n",
    "    / \"raw\"\n",
    "    / \"moz\"\n",
    "    / \"cod_ab\"\n",
    "    / \"moz_admbnda_adm1_ine_20190607.shp\"\n",
    ")\n",
    "\n",
    "gdf_adm1 = gpd.read_file(adm1_path)\n",
    "gdf_sel = gdf_adm1[gdf_adm1.ADM1_PT.isin(ADMS)]\n",
    "ibtracs_path = (\n",
    "    Path(AA_DATA_DIR)\n",
    "    / \"public\"\n",
    "    / \"raw\"\n",
    "    / \"glb\"\n",
    "    / \"ibtracs\"\n",
    "    / \"IBTrACS.SI.list.v04r01.points/IBTrACS.SI.list.v04r01.points.shp\"\n",
    ")\n",
    "adm2_path = (\n",
    "    AA_DATA_DIR\n",
    "    / \"public\"\n",
    "    / \"raw\"\n",
    "    / \"moz\"\n",
    "    / \"cod_ab\"\n",
    "    / \"moz_admbnda_adm2_ine_20190607.shp\"\n",
    ")\n",
    "\n",
    "gdf_adm2 = gpd.read_file(adm2_path)\n",
    "gdf_sel_adm2 = gdf_adm2[gdf_adm2.ADM1_PT.isin(ADMS)]\n",
    "\n",
    "minx, miny, maxx, maxy = gdf_sel.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blob_names = existing_files = [\n",
    "#    x.name for x in dev_glb_container_client.list_blobs(name_starts_with=\"imerg/v6/\")\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_ibtracs = gpd.read_file(ibtracs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_adm1_sel_buff = gdf_adm1[gdf_adm1.ADM1_PT.isin(ADMS)].buffer(250 / 111)\n",
    "# also making sure to take one time step before landfall since some storms even off shore can cause a lot of rain\n",
    "gdf_ibtracs_time = gdf_ibtracs[gdf_ibtracs[\"ISO_TIME\"] >= \"2003-03-11\"]\n",
    "# which cyclones made landfall or came close by around 50km to land\n",
    "landfall_cyclones = gpd.sjoin(\n",
    "    gdf_ibtracs_time, gdf_sel, how=\"inner\", predicate=\"intersects\"\n",
    ")[\"NAME\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# das = []\n",
    "# for blob_name in tqdm.tqdm(blob_names):\n",
    "#    cog_url = (\n",
    "#        f\"https://{DEV_BLOB_NAME}.blob.core.windows.net/global/\"\n",
    "#        f\"{blob_name}?{DEV_BLOB_SAS}\"\n",
    "#    )\n",
    "#    da_in = rxr.open_rasterio(cog_url, masked=True)\n",
    "#    da_in = da_in.sel(x=slice(minx, maxx), y=slice(miny, maxy))\n",
    "#    date_in = pd.to_datetime(blob_name.split(\".\")[0][-10:])\n",
    "#    da_in[\"date\"] = date_in\n",
    "\n",
    "#    # Persisting to reduce the number of downstream Dask layers\n",
    "#    da_in = da_in.persist()\n",
    "#    das.append(da_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = []\n",
    "dates = pd.date_range(start=\"2003-03-11\", periods=len(das), freq=\"D\")\n",
    "\n",
    "for cyc in landfall_cyclones:\n",
    "    cyc_df = gdf_ibtracs_time[gdf_ibtracs_time[\"NAME\"] == cyc]\n",
    "    cyc_df[\"date\"] = pd.to_datetime(cyc_df[\"ISO_TIME\"]).dt.date\n",
    "    cyc_sjoin = gpd.sjoin(cyc_df, gdf_sel, how=\"left\", predicate=\"within\")\n",
    "    cyc_df[\"ADM1_PT\"] = cyc_sjoin[\"ADM1_PT\"]\n",
    "    cyc_df[\"actual_within_land\"] = cyc_sjoin[\"index_right\"].notna()\n",
    "    cyc_df[\"point_location\"] = np.where(\n",
    "        cyc_df[\"actual_within_land\"], \"Within\", \"Outside\"\n",
    "    )\n",
    "    first_landfall = (\n",
    "        cyc_df[cyc_df[\"actual_within_land\"]].index[0]\n",
    "        if not cyc_df[cyc_df[\"actual_within_land\"]].empty\n",
    "        else None\n",
    "    )\n",
    "    if first_landfall is not None:\n",
    "        cyc_df.loc[cyc_df.index == first_landfall, \"point_location\"] = \"Landfall\"\n",
    "        landfall_time = pd.to_datetime(\n",
    "            cyc_df[cyc_df[\"point_location\"] == \"Landfall\"][\"ISO_TIME\"].values[0]\n",
    "        )\n",
    "        lf_dt = cyc_df[cyc_df[\"point_location\"] == \"Landfall\"]\n",
    "\n",
    "        storm_df = pd.DataFrame(\n",
    "            {\n",
    "                \"storm\": cyc,\n",
    "                \"date\": pd.date_range(\n",
    "                    start=landfall_time - pd.Timedelta(days=2),\n",
    "                    end=landfall_time + pd.Timedelta(days=5),\n",
    "                ),\n",
    "                \"time_step\": range(-2, 6),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Initialize a list to store extracted values for each row\n",
    "        extracted_values_list = []\n",
    "\n",
    "        # Iterate over each row in the DataFrame\n",
    "        for index, row in storm_df.iterrows():\n",
    "            target_date = row[\"date\"].normalize()  # Ensure target_date is a Timestamp\n",
    "            # Create a GeoDataFrame\n",
    "            gdf_lf = gpd.GeoDataFrame(\n",
    "                {\"geometry\": [Point(lf_dt[\"LON\"].values[0], lf_dt[\"LAT\"].values[0])]},\n",
    "                crs=CRS(\"EPSG:4326\"),\n",
    "            )\n",
    "            gdf_lf[\"geometry\"] = gdf_lf.buffer(250 / 111)\n",
    "            gdf_lf = gpd.overlay(gdf_lf, gdf_sel, how=\"intersection\")\n",
    "\n",
    "            # Find the index of the target_date in the dates list\n",
    "            target_timestamp = pd.Timestamp(target_date)  # Convert to Timestamp\n",
    "\n",
    "            # Get the corresponding DataArray\n",
    "            blob_name = (\n",
    "                f\"imerg/v6/imerg-daily-late-{target_date.strftime('%Y-%m-%d')}.tif\"\n",
    "            )\n",
    "            cog_url = (\n",
    "                f\"https://{DEV_BLOB_NAME}.blob.core.windows.net/global/\"\n",
    "                f\"{blob_name}?{DEV_BLOB_SAS}\"\n",
    "            )\n",
    "            # da_in = rxr.open_rasterio(cog_url, masked=True)\n",
    "            # da_in = da_in.sel(x=slice(minx, maxx), y=slice(miny, maxy))\n",
    "            # date_in = pd.to_datetime(blob_name.split(\".\")[0][-10:])\n",
    "            # da_in[\"date\"] = date_in\n",
    "\n",
    "            # Persisting to reduce the number of downstream Dask layers\n",
    "            # da_in = da_in.persist()\n",
    "            # Load the raster data\n",
    "            try:\n",
    "                da_in = rxr.open_rasterio(cog_url, masked=True)\n",
    "                # Persist the DataArray in memory\n",
    "                da_in = da_in.persist()\n",
    "                # Ensure CRS is set\n",
    "                if da_in.rio.crs is None:\n",
    "                    da_in.rio.write_crs(\n",
    "                        \"EPSG:4326\", inplace=True\n",
    "                    )  # Set the correct EPSG code if needed\n",
    "\n",
    "                # Load the polygon data\n",
    "                polygon_gdf = gdf_lf\n",
    "\n",
    "                # Create a mask from the polygon\n",
    "                polygon_union = polygon_gdf.unary_union\n",
    "                mask = geometry_mask(\n",
    "                    [polygon_union],\n",
    "                    transform=da_in.rio.transform(),\n",
    "                    invert=True,\n",
    "                    out_shape=da_in.rio.shape,\n",
    "                )\n",
    "\n",
    "                # Apply the mask to the DataArray\n",
    "                masked_da = da_in.where(mask)\n",
    "\n",
    "                # Compute the median of the values within the polygon\n",
    "                values = masked_da.values[~np.isnan(masked_da.values)]\n",
    "                median_value = np.median(values)\n",
    "            except Exception as e:\n",
    "                median_value = np.nan\n",
    "            # Append the median value to the list\n",
    "            extracted_values_list.append(median_value)\n",
    "\n",
    "        # Add the extracted values to the DataFrame\n",
    "        storm_df[\"median_precip_250km\"] = extracted_values_list\n",
    "        combined_df.append(storm_df)\n",
    "\n",
    "# Combine all DataFrames into a single DataFrame\n",
    "rain_df = pd.concat(combined_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df.to_csv(\n",
    "    AA_DATA_DIR / \"public\" / \"processed\" / \"moz\" / \"daily_imerg_cyclone_landfall.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = xr.concat(das, dim=\"date\", join=\"override\", combine_attrs=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now clip to the specific geometry\n",
    "#ds = ds.rio.write_crs(4326)\n",
    "#ds = ds.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\")\n",
    "#ds_clip = ds.rio.clip(gdf_sel.geometry)\n",
    "#results = []\n",
    "\n",
    "# TODO: Is there a better way to aggregate here?\n",
    "# Pauline's suggestion: We would be looking at landfall point so best to leave as clipped rasters. \n",
    "# We can have a trigger as a radius around the point only and not by admin level. \n",
    "# These loops will be v slow...\n",
    "#for day in ds_clip.date.values:\n",
    "\n",
    "#    ds_time = ds_clip.sel(date=day)\n",
    "\n",
    "#    for idx, row in gdf_sel.iterrows():\n",
    "#        admin_name = row[\"ADM1_PT\"]\n",
    "#        polygon = row[\"geometry\"]\n",
    "\n",
    "#        ds_clipped = ds_time.rio.clip([polygon], all_touched=True)\n",
    "#        total_precipitation = int(ds_clipped.sum(dim=[\"x\", \"y\"]).values[0])\n",
    "\n",
    "#        results.append(\n",
    "#            {\n",
    "#                \"ADM1\": admin_name,\n",
    "#                \"date\": pd.to_datetime(day),\n",
    "#                \"total_precipitation\": total_precipitation,\n",
    "#            }\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_precipitation = pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot to sanity check\n",
    "#plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Group by ADM1 and plot each group\n",
    "#for adm1, group in df_precipitation.groupby(\"ADM1\"):\n",
    "#    plt.plot(group[\"date\"], group[\"total_precipitation\"], label=adm1)\n",
    "\n",
    "#plt.xlabel(\"Date\")\n",
    "##plt.ylabel(\"Precipitation\")\n",
    "#plt.title(\"Total daily precipitation per Province in Mozambique\")\n",
    "#plt.legend(title=\"ADM1\")\n",
    "#plt.tight_layout()\n",
    "#plt.grid(False)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now further check by plotting some specific dates\n",
    "# Just observationally, the plots here make sense with the aggregations plotted above\n",
    "# ds_clip.plot(x=\"x\", y=\"y\", col=\"date\", col_wrap=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_precipitation.to_csv(\n",
    "#    AA_DATA_DIR / \"public\" / \"processed\" / \"moz\" / \"daily_imerg_precip_adm1_sel.csv\"\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
