{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and Processing ECMWF hindcasts \n",
    "\n",
    "## Check for possibility of a readiness trigger with a longer lead time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current forecast (Meteo France La Reunion) provides limited lead time and we should explore if we could use ECMWF to have a readiness signal. I would basically check some basic perfomance metrics vs lead time and decide with partners what is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyter_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyter_black\n"
     ]
    }
   ],
   "source": [
    "%load_ext jupyter_black\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from src.datasources import rsmc\n",
    "from src import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA_DATA_DIR = constants.AA_DATA_DIR\n",
    "save_dir = (\n",
    "    Path(AA_DATA_DIR) / \"public\" / \"exploration\" / \"moz\" / \"ecmwf_hindcast\"\n",
    ")\n",
    "ADMS = constants.ADMS\n",
    "adm1_path = (\n",
    "    Path(AA_DATA_DIR)\n",
    "    / \"public\"\n",
    "    / \"raw\"\n",
    "    / \"moz\"\n",
    "    / \"cod_ab\"\n",
    "    / \"moz_admbnda_adm1_ine_20190607.shp\"\n",
    ")\n",
    "gdf_adm1 = gpd.read_file(adm1_path)\n",
    "gdf_sel = gdf_adm1[gdf_adm1.ADM1_PT.isin(ADMS)]\n",
    "ibtracs_path = adm1_path = (\n",
    "    Path(AA_DATA_DIR) / \"public\" / \"raw\" / \"glb\" / \"ibtracs\"\n",
    ")\n",
    "points_path = Path(\n",
    "    ibtracs_path\n",
    "    / \"IBTrACS.SI.list.v04r01.points/IBTrACS.SI.list.v04r01.points.shp\"\n",
    ")\n",
    "gdf_points = gpd.read_file(points_path)\n",
    "gdf_points[\"Date\"] = [\n",
    "    datetime.strptime(dt, \"%Y-%m-%d %H:%M:%S\").date()\n",
    "    for dt in gdf_points[\"ISO_TIME\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_points_moz = gpd.sjoin(gdf_sel, gdf_points, predicate=\"intersects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_points_moz_2006 = gdf_points_moz[gdf_points_moz[\"year\"] >= 2006]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['720052006', 'FAVIO', 'JAYA', 'JOKWE', 'IZILDA', 'DANDO', 'IRINA',\n",
       "       'HARUNA', 'DELIWE', '920132014', 'GUITO', 'HELLEN', 'CHEDZA',\n",
       "       'DINEO', '420172018', 'DESMOND', 'IDAI', 'KENNETH', 'CHALANE',\n",
       "       'ELOISE', 'GUAMBE', 'ANA', 'DUMAKO', 'GOMBE', 'JASMINE', 'FREDDY'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading all actual cyclone tracks\n",
    "cyclone_tracks = pd.read_csv(\n",
    "    rsmc.DATA_DIR\n",
    "    / \"private\"\n",
    "    / \"raw\"\n",
    "    / \"moz\"\n",
    "    / \"rsmc\"\n",
    "    / \"data_cyclone_SWIO_19851986_to_20222023.csv\"\n",
    ")\n",
    "cyclone_tracks[\"Lat\"] = cyclone_tracks[\"Lat\"].apply(\n",
    "    lambda x: -x if x > 0 else x\n",
    ")\n",
    "cyclone_tracks[\"geometry\"] = cyclone_tracks.apply(\n",
    "    lambda row: Point(row[\"Lon\"], row[\"Lat\"]), axis=1\n",
    ")\n",
    "cyclone_tracks_gdf = gpd.GeoDataFrame(\n",
    "    cyclone_tracks, geometry=\"geometry\", crs=\"EPSG:4326\"\n",
    ")\n",
    "cyclone_tracks_sel = gpd.sjoin(\n",
    "    cyclone_tracks_gdf, gdf_sel, how=\"inner\", predicate=\"intersects\"\n",
    ")\n",
    "cyclone_tracks_sel[cyclone_tracks_sel[\"Year\"] >= 2006][\"Name\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting dates for cyclones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_dates = (\n",
    "    gdf_points[gdf_points[\"SID\"].isin(gdf_points_moz_2006[\"SID\"])]\n",
    "    .groupby([\"SID\", \"NAME\"])[\"Date\"]\n",
    "    .agg([\"min\", \"max\"])\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007043S11071</td>\n",
       "      <td>FAVIO</td>\n",
       "      <td>2007-02-11</td>\n",
       "      <td>2007-02-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008062S10064</td>\n",
       "      <td>JOKWE</td>\n",
       "      <td>2008-03-02</td>\n",
       "      <td>2008-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009082S16039</td>\n",
       "      <td>IZILDA</td>\n",
       "      <td>2009-03-22</td>\n",
       "      <td>2009-03-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012010S24049</td>\n",
       "      <td>DANDO</td>\n",
       "      <td>2012-01-10</td>\n",
       "      <td>2012-01-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012056S13057</td>\n",
       "      <td>IRINA</td>\n",
       "      <td>2012-02-25</td>\n",
       "      <td>2012-03-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013046S20042</td>\n",
       "      <td>HARUNA</td>\n",
       "      <td>2013-02-14</td>\n",
       "      <td>2013-02-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2014015S16043</td>\n",
       "      <td>DELIWE</td>\n",
       "      <td>2014-01-14</td>\n",
       "      <td>2014-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2014048S16039</td>\n",
       "      <td>GUITO</td>\n",
       "      <td>2014-02-17</td>\n",
       "      <td>2014-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2014086S10041</td>\n",
       "      <td>HELLEN</td>\n",
       "      <td>2014-03-26</td>\n",
       "      <td>2014-04-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015013S18038</td>\n",
       "      <td>CHEDZA</td>\n",
       "      <td>2015-01-13</td>\n",
       "      <td>2015-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017043S19040</td>\n",
       "      <td>DINEO</td>\n",
       "      <td>2017-02-11</td>\n",
       "      <td>2017-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019018S24033</td>\n",
       "      <td>DESMOND</td>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>2019-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019063S18038</td>\n",
       "      <td>IDAI</td>\n",
       "      <td>2019-03-04</td>\n",
       "      <td>2019-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019112S10053</td>\n",
       "      <td>KENNETH</td>\n",
       "      <td>2019-04-21</td>\n",
       "      <td>2019-04-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020355S11065</td>\n",
       "      <td>CHALANE</td>\n",
       "      <td>2020-12-20</td>\n",
       "      <td>2021-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021012S12086</td>\n",
       "      <td>ELOISE</td>\n",
       "      <td>2021-01-11</td>\n",
       "      <td>2021-01-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021042S23040</td>\n",
       "      <td>GUAMBE</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>2021-02-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022020S13059</td>\n",
       "      <td>ANA</td>\n",
       "      <td>2022-01-20</td>\n",
       "      <td>2022-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022065S16055</td>\n",
       "      <td>GOMBE</td>\n",
       "      <td>2022-03-06</td>\n",
       "      <td>2022-03-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022110S12051</td>\n",
       "      <td>JASMINE</td>\n",
       "      <td>2022-04-20</td>\n",
       "      <td>2022-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2023061S22036</td>\n",
       "      <td>FREDDY</td>\n",
       "      <td>2023-03-02</td>\n",
       "      <td>2023-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2024071S20039</td>\n",
       "      <td>FILIPO</td>\n",
       "      <td>2024-03-10</td>\n",
       "      <td>2024-03-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              SID     NAME         min         max\n",
       "0   2007043S11071    FAVIO  2007-02-11  2007-02-23\n",
       "1   2008062S10064    JOKWE  2008-03-02  2008-03-16\n",
       "2   2009082S16039   IZILDA  2009-03-22  2009-03-29\n",
       "3   2012010S24049    DANDO  2012-01-10  2012-01-18\n",
       "4   2012056S13057    IRINA  2012-02-25  2012-03-12\n",
       "5   2013046S20042   HARUNA  2013-02-14  2013-02-28\n",
       "6   2014015S16043   DELIWE  2014-01-14  2014-01-22\n",
       "7   2014048S16039    GUITO  2014-02-17  2014-02-24\n",
       "8   2014086S10041   HELLEN  2014-03-26  2014-04-05\n",
       "9   2015013S18038   CHEDZA  2015-01-13  2015-01-22\n",
       "10  2017043S19040    DINEO  2017-02-11  2017-02-17\n",
       "11  2019018S24033  DESMOND  2019-01-17  2019-01-22\n",
       "12  2019063S18038     IDAI  2019-03-04  2019-03-16\n",
       "13  2019112S10053  KENNETH  2019-04-21  2019-04-28\n",
       "14  2020355S11065  CHALANE  2020-12-20  2021-01-04\n",
       "15  2021012S12086   ELOISE  2021-01-11  2021-01-27\n",
       "16  2021042S23040   GUAMBE  2021-02-11  2021-02-22\n",
       "17  2022020S13059      ANA  2022-01-20  2022-01-25\n",
       "18  2022065S16055    GOMBE  2022-03-06  2022-03-12\n",
       "19  2022110S12051  JASMINE  2022-04-20  2022-05-01\n",
       "20  2023061S22036   FREDDY  2023-03-02  2023-03-14\n",
       "21  2024071S20039   FILIPO  2024-03-10  2024-03-14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Hindcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://data.rda.ucar.edu/ds330.3/ecmf/\"\n",
    "times = [\"000000\", \"120000\"]\n",
    "date_lists = []\n",
    "for index, row in min_max_dates.iterrows():\n",
    "    min_date = row[\"min\"]\n",
    "    max_date = row[\"max\"]\n",
    "    date_list = pd.date_range(start=min_date, end=max_date).tolist()\n",
    "    for date in date_list:\n",
    "        date = date.strftime(\"%Y%m%d\")\n",
    "        year = date[0:4]\n",
    "        for time in times:\n",
    "            server = \"test\"\n",
    "            if date >= 20080801:\n",
    "                server = \"prod\"\n",
    "            filename = f\"z_tigge_c_ecmf_{date}{time}_ifs_glob_{server}_all_glo.xml\"\n",
    "            filename_url = f\"{base_url}{year}/{date}/{filename}\"\n",
    "            print(\"Downloading\", filename)\n",
    "            req = requests.get(filename_url, allow_redirects=True)\n",
    "            open(Path(save_dir) / \"xml\" / filename, \"wb\").write(req.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing hindcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml2csv(filename):\n",
    "    print(f\"{filename}\")\n",
    "    try:\n",
    "        tree = ET.parse(filename)\n",
    "    except ET.ParseError:\n",
    "        print(\"Error with file, skipping\")\n",
    "        return\n",
    "    root = tree.getroot()\n",
    "\n",
    "    prod_center = root.find(\"header/productionCenter\").text\n",
    "    baseTime = root.find(\"header/baseTime\").text\n",
    "\n",
    "    # Create one dictonary for each time point, and append it to a list\n",
    "    for members in root.findall(\"data\"):\n",
    "        mtype = members.get(\"type\")\n",
    "        if mtype not in [\"forecast\", \"ensembleForecast\"]:\n",
    "            continue\n",
    "        for members2 in members.findall(\"disturbance\"):\n",
    "            cyclone_name = [\n",
    "                name.text.lower().strip() for name in members2.findall(\"cycloneName\")\n",
    "            ]\n",
    "            if not cyclone_name:\n",
    "                continue\n",
    "            cyclone_name = cyclone_name[0].lower()\n",
    "            if cyclone_name not in list(min_max_dates[\"NAME\"].str.lower()):\n",
    "                continue\n",
    "            print(f\"Found cyclone {cyclone_name}\")\n",
    "            for members3 in members2.findall(\"fix\"):\n",
    "                tem_dic = {}\n",
    "                tem_dic[\"mtype\"] = [mtype]\n",
    "                tem_dic[\"product\"] = [re.sub(\"\\\\s+\", \" \", prod_center).strip().lower()]\n",
    "                tem_dic[\"cyc_number\"] = [\n",
    "                    name.text for name in members2.findall(\"cycloneNumber\")\n",
    "                ]\n",
    "                tem_dic[\"ensemble\"] = [members.get(\"member\")]\n",
    "                tem_dic[\"speed\"] = [\n",
    "                    name.text\n",
    "                    for name in members3.findall(\"cycloneData/maximumWind/speed\")\n",
    "                ]\n",
    "                tem_dic[\"pressure\"] = [\n",
    "                    name.text\n",
    "                    for name in members3.findall(\"cycloneData/minimumPressure/pressure\")\n",
    "                ]\n",
    "                time = [name.text for name in members3.findall(\"validTime\")]\n",
    "                tem_dic[\"time\"] = [\n",
    "                    \"/\".join(time[0].split(\"T\")[0].split(\"-\"))\n",
    "                    + \", \"\n",
    "                    + time[0].split(\"T\")[1][:-1]\n",
    "                ]\n",
    "                tem_dic[\"lat\"] = [name.text for name in members3.findall(\"latitude\")]\n",
    "                tem_dic[\"lon\"] = [name.text for name in members3.findall(\"longitude\")]\n",
    "                tem_dic[\"lead_time\"] = [members3.get(\"hour\")]\n",
    "                tem_dic[\"forecast_time\"] = [\n",
    "                    \"/\".join(baseTime.split(\"T\")[0].split(\"-\"))\n",
    "                    + \", \"\n",
    "                    + baseTime.split(\"T\")[1][:-1]\n",
    "                ]\n",
    "                tem_dic1 = dict(\n",
    "                    [\n",
    "                        (k, \"\".join(str(e).lower().strip() for e in v))\n",
    "                        for k, v in tem_dic.items()\n",
    "                    ]\n",
    "                )\n",
    "                # Save to CSV\n",
    "                outfile = save_dir / f\"csv/{cyclone_name}_all.csv\"\n",
    "                pd.DataFrame(tem_dic1, index=[0]).to_csv(\n",
    "                    outfile,\n",
    "                    mode=\"a\",\n",
    "                    header=not os.path.exists(outfile),\n",
    "                    index=False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = sorted(list(Path(save_dir / \"xml\").glob(\"*.xml\")))\n",
    "for filename in filename_list:\n",
    "    xml2csv(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
